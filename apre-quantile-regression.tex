


% \section{Quantile Regression}\label{quantile-regression}

\begin{frame}{Definition of the Conditional Quantile}


\begin{columns}

\column{0.5\textwidth}

\tiny

Let the conditional quantile function of \(Y\) for a given value \(x\)
of the \(d\)-dimensional random variable \(X\), i.e.,
\(Q_{Y|X}:[0,1] \times \mathbb{R}^d \rightarrow \mathbb{R}\), can be
defined as:
\[Q_{Y|X}(\alpha,x) = F_{Y|X=x}^{-1}(\alpha) = \inf\{y: F_{Y|X=x}(y) \geq \alpha\}.\]

\begin{block}{Quantile function for a finite sample $\{ y_t,x_t \}_{t \in T}$}
    \begin{equation*}
        \hat{Q}_{Y|X}(\alpha,\cdot)\quad\in\quad  \underset{q\in\mathcal{Q}}{\text{arg min}}\, L_\alpha(q) = \sum_{t\in T}\rho_{\alpha}(y_{t}-q(x_t))
    \end{equation*}
    $$
    \rho_{\alpha}(x)=\begin{cases}
        \alpha x & \text{if }x\geq0\\
        (1-\alpha)x & \text{if }x<0
        \end{cases}
    $$
\end{block}

\column{0.5\textwidth}

\end{columns}

\end{frame}

% \begin{frame}{Conditional Quantile from a sample}

% Let a dataset be composed from \(\{y_t,x_t \}_{t \in T}\) and let
% \(\rho\) be the check function

% \begin{equation}\label{eq:check-function}
% \rho_{\alpha}(x)=\begin{cases}
% \alpha x & \text{if }x\geq0\\
% (1-\alpha)x & \text{if }x<0
% \end{cases},
% \end{equation}

% The sample quantile function for a given probability \(\alpha\) is then
% based on a finite number of observations and is the solution to
% minimizing the loss function \(L(\cdot)\): \[
% \hat{Q}_{Y|X}(\alpha,\cdot)\quad\in\quad  \underset{q(\cdot)\in\mathcal{Q}}{\text{arg min}}\, L_\alpha(q) = \sum_{t\in T}\rho_{\alpha}(y_{t}-q(x_t)), 
% \] \[
% q(x_t) = \beta_0 + \beta^T x_t,
% \] where \(\mathcal{Q}\) is a space of functions. In this paper, we use
% \(\mathcal{Q}\) as an \textbf{affine functions space}.

% \end{frame}

% \begin{frame}{Conditional Quantile from a sample}

% \begin{itemize}

% \item
% For a single quantile, this problem can be solved by the following
% Linear Programming problem: \[
% \begin{array}{lll}
% \underset{\beta_0, \beta,\varepsilon_{t}^{+}, \varepsilon_{t}^{-}}{\text{min}} & \sum_{t \in T} \left(\alpha \varepsilon_{t}^{+}+(1-\alpha)\varepsilon_{t}^{-}\right) & \\
% \mbox{s.t. } & \varepsilon_{t}^{+}-\varepsilon_{t}^{-}=y_{t} - \beta_{0} - \beta^T x_{t}, & \qquad\forall t \in T,\\
% & \varepsilon_t^+,\varepsilon_t^- \geq 0, & \qquad \forall t \in T.
% \end{array}
% \]
% \item
% The output are the coefficients \(\beta_0\) and \(\beta\) (which is
% the same dimension as \(x_t\)), that describe the quantile function as
% an affine function.
% \end{itemize}

% \end{frame}

% \begin{frame}{The non-crossing issue}

% \begin{itemize}
% 	\item The following condition must always hold:
% 	$$q_\alpha(x_t) \leq q_{\alpha'}(x_t) \text{, when } \alpha \leq \alpha'$$
% \end{itemize}

% \begin{figure}
% \centering
% \begin{minipage}[t]{\linewidth}
% \centering
% \begin{minipage}[t]{0.45\linewidth}
% \centering     \includegraphics[width=\textwidth]{Images/icaraizinho-quantile-linear-scatter-crossing}
% \subcaption{Each $\alpha$-quantile estimated independently}
% \end{minipage}
% \begin{minipage}[t]{0.45\linewidth}
% \centering     \includegraphics[width=\textwidth]{Images/icaraizinho-quantile-linear-scatter}
% \subcaption{Estimation with non-crossing constraint}
% \end{minipage}
% \end{minipage}
% \caption{These graphs show how the addition of a constraint can contour the crossing quantile issue}
% \label{fig:quantiles-vs-xt}
% \end{figure}

% \end{frame}

% \begin{frame}{Notation}

% \small

% \begin{longtable}[]{@{}ll@{}}
% \toprule
% \begin{minipage}[b]{0.14\columnwidth}\raggedright\strut
% Expression\strut
% \end{minipage} & \begin{minipage}[b]{0.80\columnwidth}\raggedright\strut
% Meaning\strut
% \end{minipage}\tabularnewline
% \midrule
% \endhead
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(Q_{Y \mid X}(\alpha,x)\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% The conditional quantile function\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(y_t\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% the time series we are modelling\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(x_t\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% explanatory variables of \(y_t\) in \(t\)\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(T\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% the set containing all observations indexes\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(J\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% the set containing all quantile indexes\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(J_{(-1)}\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% the set \(J\backslash \{1\}\)\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(\alpha_j\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% a probability, might be indexed by \(j\)\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(A\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% the set of probabilities \(\{\alpha_j \mid j \in J\}\)\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(K\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% Maximum number of covariates on MILP regularization\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(\lambda\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% The Lasso penalization on the coefficients \(\ell_1\)-norm\strut
% \end{minipage}\tabularnewline
% \begin{minipage}[t]{0.14\columnwidth}\raggedright\strut
% \(\gamma\)\strut
% \end{minipage} & \begin{minipage}[t]{0.80\columnwidth}\raggedright\strut
% The penalization on the coefficients second-derivative with respect of
% the quantiles\strut
% \end{minipage}\tabularnewline
% \bottomrule
% \end{longtable}

% \end{frame}

% \begin{frame}{Conditional Quantile as a Linear Programming Problem}

% \[
% \min_{\beta_{0j},\beta_j,\varepsilon_{tj}^{+}, \varepsilon_{tj}^{-}} \, \sum_{j \in J} \sum_{t \in T}\left(\alpha_j \varepsilon_{t j}^{+}+(1-\alpha_j)\varepsilon_{t j}^{-}\right)
% \] \[
% \begin{array}{lr}
% \text{s.t.} &\\
% \varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}=y_{t} - \beta_{0j} - \beta_{j}^T x_{t}, & \forall t \in T, \forall j \in J, \\
% \varepsilon_{tj}^+,\varepsilon_{tj}^- \geq 0, & \forall t \in T,\forall j \in J,\\
% \beta_{0,j-1} + \beta_{j-1}^T x_{t} \leq \beta_{0j} + \beta_{j}^T x_{t},
% & \forall t \in T, \forall j \in J_{(-1)},
% \end{array}
% \]

% \begin{itemize}
% \item
% Coefficients \(\beta_{0j}\) and \(\beta_j\) refer to the
% \(j\)\textsuperscript{th} quantile
% \item
% We apply QR to estimate the conditional distribution
% \(\hat{Q}_{Y_{t+h}|X_{t+h},Y_t, Y_{t-1}, \dots} (\alpha,\cdot)\) for a
% \(k\)-step ahead forecast of time serie \(\{y_t\}\), where \(X_{t+h}\)
% is a vector of exogenous variables at the time we want to forecast.
% \end{itemize}

% \end{frame}

% \section{Regularization of
% covariates}\label{regularization-of-covariates}

% \begin{frame}{Best Subset selection via MILP}

% \begin{itemize}

% \item
% Mixed Integer Linear Programming (MILP) models allow only \(K\)
% variables to be used for each \(\alpha\)-quantile.
% \item
% Only \(K\) coefficients \(\beta_{pj}\) may have nonzero values, for
% each \(\alpha\)-quantile.
% \item
% It is guaranteed by constraints on the optimization model.
% \item
% One model for each \(\alpha\)-quantile
% \end{itemize}

% \end{frame}

% \begin{frame}{Best Subset selection via MILP}


% \begin{IEEEeqnarray*}{llr}
% \underset{\beta_{0j},\beta_j,z_{p j}, \varepsilon_{t j}^{+},\varepsilon_{t j}^{-}}{\text{min}} & \sum_{j \in J} \sum_{t\in T}\left(\alpha_j\varepsilon_{t j}^{+}+(1-\alpha_j)\varepsilon_{t j}^{-}\right)  & \\
% \mbox{s.t } & \varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}=y_{t}-\beta_{0 j}-\beta_{j}^T x_{t},& \forall t \in T ,\forall j \in J, \\
% & \varepsilon_{t j}^{+},\varepsilon_{t j}^{-}\geq0,&\forall t \in T ,\forall j \in J, \\
% & - M z_{p j} \leq \beta_{p j} \leq M z_{p j},& \forall j \in J, \forall p\in P, \\
% & \sum_{p \in P} z_{p j} \leq K, &  \forall j \in J, \\
% & z_{p j} \in \{0,1\},& \forall j \in J, \forall p\in P,\\
% & \beta_{0,j-1} + \beta_{j-1}^T x_{t} \leq \beta_{0j} + \beta_{j}^T x_{t}, & \quad \forall t \in T, \forall j \in J_{(-1)},
% \end{IEEEeqnarray*}


% \begin{itemize}

% \item
% \(z_{pj}\) is a binary variable which indicates when
% \(\beta_{pj} > 0\).
% \end{itemize}

% \end{frame}

% \begin{frame}{Variable Selection via LASSO}

% \begin{itemize}

% \item
% Regularization by including the coefficients \(\ell_1\)-norm on the
% objective function.
% \item
% In this method, coefficients are shrunk towards zero by changing a
% continuous parameter \(\lambda\), which penalizes the size of the
% \(\ell_1\)-norm.\\
% \item
% When the value of \(\lambda\) gets bigger, fewer variables are
% selected to be used.
% \item
% The optimization problem for a single quantile is presented below: 
% \[
% \underset{\beta_{0},\beta}{\text{min}} \sum_{t\in T}\rho_{\alpha}(y_{t}-(\beta_0 + \beta^T x_t))+\lambda\|\beta\|_{1},
% \]
% \end{itemize}

% \end{frame}

% \begin{frame}{Variable Selection via LASSO}

% \begin{itemize}

% \item
% At first, we select variables using LASSO
% \end{itemize}

% \begin{IEEEeqnarray*}{lr}
% \underset{\beta_{0},\beta,\varepsilon_{t j}^{+},\varepsilon_{t j}^{-}}{\text{arg min}} \sum_{j \in J} \sum_{t \in T}\left(\alpha_j \varepsilon_{t j}^{+}+(1-\alpha_j)\varepsilon_{t j}^{-}\right)+\lambda\sum_{p \in P}\mbox{\ensuremath{\xi}}_{p j} \span \label{eq:obj-lasso} \\
% \mbox{subject to} \span \\
% \varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}= y_{t}-\beta_{0 j}-\beta_{j}^T x_{t},&\forall t\in T, \forall j \in J, \\
% \varepsilon_{t j}^{+},\varepsilon_{t j}^{-}\geq0,&\forall t \in T, \forall j \in J,\\
% \xi_{p\alpha}\geq\beta_{p j},&\forall p\in P, \forall j \in J, 
% \\
% \xi_{p\alpha}\geq-\beta_{p j},&\forall p\in P, \forall j \in J\\
% \beta_{0,j-1} + \beta_{j-1}^T x_{t} \leq \beta_{0j} + \beta_{j}^T x_{t}, & \quad \forall t \in T, \forall j \in J_{(-1)},\\
% \end{IEEEeqnarray*}

% \end{frame}

% \begin{frame}{Variable Selection via LASSO}

% \begin{itemize}
% \item
% We then define \(S_\lambda\) as the set of indexes of selected
% variables given by \[
% S_{\lambda} = \{ p \in \{ 1,\dots,P \} | \; |\beta^{*LASSO}_{\lambda,p}| \neq 0  \}.
% \] Hence, we have that, for each \(p \in \{ 1,\dots,P \}\),
% \[\beta^{*LASSO}_{\theta,p} = 0 \Longrightarrow \beta^{*}_{\theta,p} = 0.\]
% \item
% On the second stage, we estimate coefficients using a regular QR where
% input variables are only the ones which belonging to \(S_\lambda\)
% \end{itemize}

% \end{frame}

% \section{Regularization on the
% quantiles}\label{regularization-on-the-quantiles}

% \begin{frame}{Regularization on the quantiles - Motivation}
% 	In practice, often when we regularize 
% 	\begin{figure}
% 		\centering
% 		\includegraphics[width=0.5\linewidth]{Images/Lambda10-gamma03.pdf}
% 	\end{figure}
	
% \end{frame}


% \begin{frame}{MILP - Defining groups for \(\alpha\)-quantiles}


% \begin{IEEEeqnarray*}{lll}
% \underset{\beta_{0j},\beta_j,z_{p j}, \varepsilon_{t j}^{+},\varepsilon_{t j}^{-}}{\text{min}} & \sum_{j \in J} \sum_{t\in T}\left(\alpha_j\varepsilon_{t j}^{+}+(1-\alpha_j)\varepsilon_{t j}^{-}\right)  & \\
% \mbox{s.t } & \varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}=y_{t}-\beta_{0 j}-\beta_{j}^T x_{t},& \forall t \in T ,\forall j \in J, \\
% & \varepsilon_{t j}^{+},\varepsilon_{t j}^{-}\geq0,&\forall t \in T ,\forall j \in 
% J, \\
% &\mathcal{Z}_{p j g} := 2 - ( 1-z_{pg}) - I_{gj}, & \\
% & - M \mathcal{Z}_{p j g} \leq \beta_{p j} \leq M \mathcal{Z}_{p j g},& \forall j \in J, \forall p\in P, \forall g \in G   \\
% & \sum_{p \in P} z_{p g} \leq K, &  \forall j \in J, \\
% & \beta_{0,j-1} + \beta_{j-1}^T x_{t} \leq \beta_{0j} + \beta_{j}^T x_{t}, & \forall t \in T, \forall j \in J_{(-1)},\\
% & I_{gj}, z_{pg} \in \{0,1\},& \forall p \in P,\forall g \in G, \\
% & z_{p g} \in \{0,1\},& \forall j \in J, \forall p\in P,\\
% \end{IEEEeqnarray*}


% \end{frame}




% \begin{frame}{MILP with quantile regularization}

% \small
% \begin{IEEEeqnarray*}{lr}
% 	\underset{\beta_{0j},\beta_j,z_{p j} \varepsilon_{t j}^{+},\varepsilon_{t j}^{-}}{\text{min}} \sum_{j \in J} \sum_{t\in T}\left(\alpha_j\varepsilon_{t j}^{+}+(1-\alpha_j)\varepsilon_{tj}^{-}\right) \nonumber \span \\
% 	\span + \gamma \sum_{j \in J'} (D2_{pj}^+ + D2_{pj}^-)   \\
% 	\mbox{subject to} \span \nonumber \\
% 	\varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}=y_{t}-\beta_{0 j}-\beta_{j}^T x_{t,p},& \forall t \in T ,\forall j \in J,\\
% 	- M z_{p \alpha} \leq \beta_{p j} \leq M z_{p \alpha}, & \forall j \in J, \forall p\in P, \\
% 	\sum_{p \in P} z_{p \alpha} \leq K, & \forall j \in J, \\
% 	D2_{pj}^+ - D2_{pj}^- = \frac{\left(\frac{\beta_{p,j+1}-\beta_{pj}}{\alpha_{j+1}-\alpha_{j}}\right)-\left(\frac{\beta_{p,j}-\beta_{p,j-1}}{\alpha_{j}-\alpha_{j-1}}\right)}{\alpha_{j+1}-2\alpha_{j}+\alpha_{j-1}}, \span   \nonumber \\
% 	\span \forall j \in J_{(-1)}, \forall p\in P, \\
% 	\beta_{0j} + \beta_{j}^T x_{t} \leq \beta_{0,j+1} + \beta_{j+1}^T x_{t},&\forall t \in T, \forall j \in J_{(-1)}, \\
% 	z_{p \alpha} \in \{0,1\}, & \forall j \in J,  \forall p\in P, \\
% 	\varepsilon_{t j}^{+},\varepsilon_{t j}^{-}\geq0, & \forall t \in T ,\forall j \in J, \\
% 	D2_{pj}^+, D2_{pj}^- \geq 0, & \forall j \in J,  \forall p\in P.
% \end{IEEEeqnarray*}

% \end{frame}

% \begin{frame}{LASSO with quantile regularization}
% \footnotesize
% \begin{IEEEeqnarray*}{lr}
% 	\tilde \beta_\lambda^{*LASSO} = \underset{\beta_{0},\beta,\varepsilon_{t j}^{+},\varepsilon_{t j}^{-}}{\text{arg min}} \sum_{j \in J} \sum_{t \in T}(\alpha_j\varepsilon_{t j}^{+}+(1-\alpha_j)\varepsilon_{t j}^{-}) \span \nonumber \\
% 	\span +\lambda\sum_{p \in P}\mbox{\ensuremath{\xi}}_{p j} + \gamma \sum_{j \in J'} (D2_{pj}^+ + D2_{pj}^-)  \label{eq:obj-lasso} \\
% 	\mbox{subject to } \nonumber & \\
% 	\varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}=y_{t}-\beta_{0 j}-\beta_{j}^T x_{t,p},& \forall t \in T ,\forall j \in J,\\
% 	\xi_{pj}\geq\beta_{p j},&\forall p\in P, \forall j \in J,  \label{l1-qar-3}
% 	\\
% 	\xi_{pj}\geq - \beta_{p j},&\forall p\in P, \forall j \in J,  \label{l1-qar-4}
% 	\\
% 	D2_{pj}^+ - D2_{pj}^- = \frac{\left(\frac{\beta_{p,j+1}-\beta_{pj}}{\alpha_{j+1}-\alpha_{j}}\right)-\left(\frac{\beta_{p,j}-\beta_{p,j-1}}{\alpha_{j}-\alpha_{j-1}}\right)}{\alpha_{j+1}-2\alpha_{j}+\alpha_{j-1}}, \span   \nonumber \\
% 	\span \forall j \in J_{(-1)}, \forall p\in P, \\
% 	\beta_{0j} + \beta_{j}^T x_{t} \leq \beta_{0,j+1} + \beta_{j+1}^T x_{t},&\forall t \in T, \forall j \in J_{(-1)}, \\
% 	\varepsilon_{t j}^{+},\varepsilon_{t j}^{-}\geq0,&\forall t \in T, \forall j \in J,\\
% 	D2_{pj}^+, D2_{pj}^- \geq 0, & \forall j \in J,  \forall p\in P. \label{eq:l1-qar5} 
% \end{IEEEeqnarray*}


% %
% %\begin{eqnarray*}
% %\underset{\beta_{0},\beta,\varepsilon_{t j}^{+},\varepsilon_{t j}^{-}}{\text{arg min}} & \sum_{j \in J} \sum_{t \in T}\left(\alpha_j \varepsilon_{t j}^{+}+(1-\alpha_j)\varepsilon_{t j}^{-}\right)+\lambda\sum_{p=1}^{P}\mbox{\ensuremath{\xi}}_{p j} + \gamma \sum_{j \in J'} D2_{pj} \span \label{eq:obj-lasso} \\
% %\mbox{s.t. } & \varepsilon_{t j}^{+}-\varepsilon_{t j}^{-}= y_{t}-\beta_{0 j}-\sum_{p=1}^{P}\beta_{p j}\tilde x_{t,p},&\forall t\in T, \forall j \in J, \\
% %& \varepsilon_{t j}^{+},\varepsilon_{t j}^{-}\geq0,&\forall t \in T, \forall j \in J,\\
% %& \xi_{p\alpha}\geq\beta_{p j},&\forall p\in P, \forall j \in J,  \label{l1-qar-3}
% %\\
% %& \tilde{D}_{pj}^{2}=\frac{\left(\frac{\beta_{p,j+1}-\beta_{pj}}{\alpha_{j+1}-\alpha_{j}}\right)-\left(\frac{\beta_{p,j}-\beta_{p,j-1}}{\alpha_{J}-\alpha_{j-1}}\right)}{\alpha_{j+1}-2\alpha_{j}+\alpha_{j-1}} \span\\
% %& D2_{pj} >  \tilde D_{pj}^{2} &  \forall j \in J_{(-1)}, \forall p\in P, \\
% %& D2_{pj} >  - \tilde D_{pj}^{2} &  \forall j \in J_{(-1)}, \forall p\in P,\\
% %& \beta_{0,j-1} + \beta_{j-1}^T x_{t} \leq \beta_{0j} + \beta_{j}^T x_{t}, & \forall t \in T, \forall j \in J_{(-1)},\\
% %& \xi_{pj}\geq-\beta_{p j},&\forall p\in P, \forall j \in J. 
% %\end{eqnarray*}

% \end{frame}

% \begin{frame}{Regularization on the quantiles}

% %\begin{figure}
% %\centering
% %\includegraphics[width=0.9\linewidth]{Images/}
% %\end{figure}

% \end{frame}

% \begin{frame}{LASSO - Penalization of derivative}

% \begin{figure}
% \centering
% \includegraphics[width=0.9\linewidth]{Images/Lambda500-gamma30.pdf}
% \end{figure}

% \end{frame}

% \begin{frame}{LASSO - Penalization of derivative}

% \begin{figure}
% \centering
% \includegraphics[width=0.7\linewidth]{Images/Lambda1000-gamma100.pdf}
% \caption{Testing caption}
% \end{figure}

% \end{frame}


% \begin{frame}{ADALASSO}

% \small
% LASSO solutions are solutions that minimize

% \[
% Q(\beta|X,y)=\frac{1}{2n}\parallel y-X\beta\parallel^{2}+\lambda\sum_{p \in P}\mid\beta_{p}\mid.
% \]
% The adaptive lasso adds weights to try to correct the known issue of bias on the LASSO estimation. The ADALASSO estimation is the second step LASSO given by:
% \[
% Q_{a}(\beta|X,y,w)=\frac{1}{2n}\parallel y-X\beta\parallel^{2}+\lambda\sum_{p \in P}w_{p}\mid\beta_{p}\mid,
% \]
% where $w_p$ weights the coefficients differently according to their first step value.  Normally, we have
% \[
% w_{p}(\lambda)=w(\tilde{\beta}_{p,t}(\lambda)).
% \]

% \end{frame}

